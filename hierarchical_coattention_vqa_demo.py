# -*- coding: utf-8 -*-
"""Hierarchical Coattention VQA -demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ewRo2d76w1fkJQUelZOGTaI0rR8T__bD
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/My\ Drive/VQA

!pip install -r requirements.txt

!pip install -r requirements.txt

!pip install silence_tensorflow

import numpy as np
import pandas as pd
import re
import glob

import os
import logging
logging.basicConfig(level=logging.INFO)

import tensorflow as tf
import silence_tensorflow.auto  # pylint: disable=unused-import
physical_devices = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)
from tensorflow.keras.applications import VGG19
from tensorflow.keras.layers import Input
from tensorflow.keras.preprocessing import sequence

from models.arch import build_model
from models.layers import ContextVector, PhraseLevelFeatures, AttentionMaps
from utils.load_pickles import tok, labelencoder
from utils.helper_functions import image_feature_extractor, process_sentence, predict_answers

from matplotlib import pyplot as plt
import matplotlib.image as mpimg
from google.colab import files



"""**Storing all the variables needed for the model**"""

max_answers = 1000
max_seq_len = 22
vocab_size  = len(tok.word_index) + 1
dim_d       = 512
dim_k       = 256
l_rate      = 1e-4
d_rate      = 0.5
reg_value   = 0.01
MODEL_PATH = 'pickles/complete_model.h5'
IMAGE_PATH = 'static'

custom_objects = {
    'PhraseLevelFeatures': PhraseLevelFeatures,
    'AttentionMaps': AttentionMaps,
    'ContextVector': ContextVector
    }

"""**Loading the saved model and the pretrained VGG19 model**"""

# load the model
model = tf.keras.models.load_model(MODEL_PATH, custom_objects=custom_objects)
vgg_model = VGG19(weights="imagenet", include_top=False, input_tensor=Input(shape=(3, 224, 224)))

"""**Predict Function**

> Extracts the question and image features given as input and passes these features onto the model.

> Output is the answer predicted by the model
"""

def predict(img,que):

  img_feat = image_feature_extractor(img, vgg_model)

  questions_processed = pd.Series(que).apply(process_sentence)

  question_data = tok.texts_to_sequences(questions_processed)
  
  question_data = sequence.pad_sequences(question_data, \
                                               maxlen=max_seq_len,\
                                               padding='post')
  
  
  y_predict = predict_answers(img_feat, question_data, model, labelencoder)
  return y_predict

"""**Pass the image and question to the predict function**"""

uploaded = files.upload()
for i in uploaded.keys():
  img = i
 
imgage=mpimg.imread(img)
imgplot = plt.imshow(imgage)

que='whatis in it'
ans = predict(img,que)

print(ans[0])

"""**Plot the image**

**Plot the image,question and answer**
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline


fig = plt.figure()
ax = fig.add_axes((0.1, 0.2, 2, 1))
a = fig.add_subplot(122)
imgplot = plt.imshow(imgage)

# a = fig.add_subplot(211)
# imgplot = fig.imshow(img)
fig.text(0.6, 0.7, que,  transform=ax.transAxes,size=22)
fig.text(0.6, 0.4, ans[0],  transform=ax.transAxes,size=22)
plt.show()

