# -*- coding: utf-8 -*-
"""Hierachical Coattention VQA Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L506vUcsOMC3t6_tMI1fj8mggQgL2VRF

# Importing libraries
"""

!pip install pyyaml h5py

import h5py

from google.colab import drive
drive.mount('/content/drive')

!pip install silence_tensorflow
!pip install tensorflow==2.1.0

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')

import tqdm as tqdm
import shutil

import time
from contextlib import contextmanager
import os, argparse
import cv2, spacy, numpy as np
# from sklearn.externals import joblib
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import re
import gc

import csv
import glob
# set the matplotlib backend so figures can be saved in the background
import matplotlib
import matplotlib.pyplot as plt
matplotlib.use("Agg")
# import the necessary packages

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn import preprocessing
from sklearn.metrics import classification_report

from imutils import paths
import math
import numpy as np
import pickle
import operator
from operator import itemgetter
from itertools import zip_longest
from collections import defaultdict
import json
import joblib
from tqdm import tqdm
import pandas as pd
from nltk.tokenize.treebank import TreebankWordTokenizer
import pandas as pd
import seaborn as sns
import datetime

# %matplotlib inline

@contextmanager
def timer(name):
    t0 = time.time()
    yield
    print(f'[{name}] done in {time.time() - t0:.0f} s')

import tensorflow as tf
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.layers import Dense,Input,LSTM,LSTMCell,Bidirectional,Activation,Conv1D,GRU, add, Conv2D, Reshape
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten
from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D, multiply
from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint
from tensorflow.keras.models import Model , load_model
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG16, VGG19
from tensorflow.keras.applications.vgg19 import preprocess_input
from tensorflow.keras.utils import Sequence
from tensorflow.keras import utils
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing import image, text, sequence
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Layer
from tensorflow.keras import backend as K
K.set_image_data_format('channels_first')

# silent tensorflow warnings
import silence_tensorflow.auto

#https://stackoverflow.com/questions/60048292/how-to-set-dynamic-memory-growth-on-tf-2-1
physical_devices = tf.config.list_physical_devices('GPU') 
tf.config.experimental.set_memory_growth(physical_devices[0], True)

import tensorflow_addons as tfa
from tensorflow_addons.metrics import F1Score

print('Using tf version: {}'.format(tf.__version__))

"""# Data preparation"""

def download_vqa_data():
    Questions_Train_mscoco = tf.keras.utils.get_file('v2_Questions_Train_mscoco.zip',
                                      cache_subdir=os.path.abspath('/content/data'),
                                      origin = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip',
                                        extract = True)

    Questions_Val_mscoco = tf.keras.utils.get_file('v2_Questions_Val_mscoco.zip',
                                        cache_subdir=os.path.abspath('/content/data'),
                                        origin = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip',
                                        extract = True)

    Annotations_Train_mscoco = tf.keras.utils.get_file('v2_Annotations_Train_mscoco.zip',
                                        cache_subdir=os.path.abspath('/content/data'),
                                        origin = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip',
                                        extract = True)

    Annotations_Val_mscoco = tf.keras.utils.get_file('v2_Annotations_Val_mscoco.zip',
                                        cache_subdir=os.path.abspath('/content/data'),
                                        origin = 'https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip',
                                        extract = True)
    os.remove(Questions_Train_mscoco)
    os.remove(Questions_Val_mscoco)
    os.remove(Annotations_Train_mscoco)
    os.remove(Annotations_Val_mscoco)

download_vqa_data()

# https://github.com/avisingh599/visual-qa/blob/99be95d61bf9302495e741fa53cf63b7e9a91a35/scripts/dumpText.py
def getAllAnswer(answers_list):
	"""
	Joins a list of answers by ";" to a single string
	
	Input:
		answer_list: list of answers (there are 10 answers for every question)

	Returns:
		A string in the format "ans1;ans2;...;ans10"
	"""
	answers = []
	for i in range(len(answers_list)):
		answers.append(answers_list[i]['answer'])
	return ';'.join(answers)

# https://github.com/shiyangdaisy23/QTA-in-VQA/blob/master/Datapreprocess/textpreprocess.py
def process_question_annotation(subset):
    """
    Creates a JSON file whose elements are (image_path, question, answer, answer_list)

    Input:
      subset: subset can be train2014 or val2014
    """
    anno = json.load(open(f'/content/data/v2_mscoco_{subset}_annotations.json', 'r'))
    ques = json.load(open(f'/content/data/v2_OpenEnded_mscoco_{subset}_questions.json', 'r'))
    imdir='%s/COCO_%s_%012d.jpg' ## COCO_train2014_000000291417.jpg
    data = []

    for i in tqdm(range(len(anno['annotations']))):
        image_path = imdir%(subset, subset, anno['annotations'][i]['image_id'])
        question = ques['questions'][i]['question']
        answer = anno['annotations'][i]['multiple_choice_answer']
        
        answer_list = getAllAnswer(anno['annotations'][i]['answers'])

        data.append({'img_path': image_path, 'question': question, 'answer': answer, 'answer_list': answer_list})
    
    # save to disk
    json.dump(data, open(f'/content/drive/My Drive/Capstone/VQA 2/data/vqa_raw_{subset}.json', 'w'))

subset = ['train2014', 'val2014']

with timer('Processing Questions and Annotations:'):
  for x in subset:
    process_question_annotation(x)

# loading from disk
train_data = json.load(open(f'/content/drive/My Drive/Capstone/VQA 2/data/vqa_raw_train2014.json', 'r'))
val_data = json.load(open(f'/content/drive/My Drive/Capstone/VQA 2/data/vqa_raw_val2014.json', 'r'))

train_data[2]['question']

val_data[0]['question']

"""## Reduce dataset to top 1000 answers"""

answer_freq= defaultdict(int)
for answer in list(map(itemgetter('answer'), train_data)):
		answer_freq[answer] += 1
print('There are total ', len(answer_freq), ' different types of answers.')

max_answers = 1000 #nb_classes

def selectTopAnswersData(questions_list, answer_list, answers_list, images_list, k):
	"""
	Reduces the train dataset to contain only the datapoints whose answer occurence is in top k 
	
	Input:
		questions_list: list of questions
		answer_list: list of most frequent answer
		answers_list: list of answers 
		images_list: list of image path
		k: number of top answers 

	Returns:
		Returns tuple of reduced dataset and the top k answers (questions_list, answer_list, answers_list, images_list, top_answers)
	"""
	answer_freq= defaultdict(int)

	for answer in answer_list:
		answer_freq[answer] += 1

	sorted_freq = sorted(answer_freq.items(), key=operator.itemgetter(1), reverse=True)[0: k]
	top_answers, top_freq = zip(*sorted_freq)
 
	new_questions_list=[]
	new_answer_list=[]
	new_answers_list=[]
	new_images_list=[]

	for question, answer, answers, image in zip(questions_list, answer_list, answers_list, images_list):
		if answer in top_answers:
			new_questions_list.append(question)
			new_answer_list.append(answer)
			new_answers_list.append(answers)
			new_images_list.append(image)
	
	print('Data size reduced by', np.round(((len(questions_list)-len(new_questions_list))/len(questions_list))*100, 2),'%')
	return(new_questions_list, new_answer_list, new_answers_list, new_images_list, top_answers)

questions_train, answer_train, answers_train, images_train, top_answers = selectTopAnswersData(list(map(itemgetter('question'), train_data)), 
                                                                                               list(map(itemgetter('answer'), train_data)), 
                                                                                               list(map(itemgetter('answer_list'), train_data)), 
                                                                                               list(map(itemgetter('img_path'), train_data)), max_answers)

print(questions_train[0])

def filterTopAnswersData(questions_list, answer_list, answers_list, images_list, top_answers):
	"""
	Reduces the validation dataset to contain only the datapoints whose answer is in top_answers 
	
	Input:
		questions_list: list of questions
		answer_list: list of most frequent answer
		answers_list: list of answers 
		images_list: list of image path
		top_answers: list of top answers

	Returns:
		Returns tuple of reduced dataset (questions_list, answer_list, answers_list, images_list)
	"""
	new_questions_list=[]
	new_answer_list=[]
	new_answers_list=[]
	new_images_list=[]

	for question, answer, answers, image in zip(questions_list, answer_list, answers_list, images_list):
		if answer in top_answers:
			new_questions_list.append(question)
			new_answer_list.append(answer)
			new_answers_list.append(answers)
			new_images_list.append(image)
	
	print('Data size reduced by', np.round(((len(questions_list)-len(new_questions_list))/len(questions_list))*100, 2),'%')
	return (new_questions_list, new_answer_list, new_answers_list, new_images_list)

questions_val, answer_val, answers_val, images_val = filterTopAnswersData(list(map(itemgetter('question'), val_data)), 
                                                                          list(map(itemgetter('answer'), val_data)),
                                                                          list(map(itemgetter('answer_list'), val_data)), 
                                                                          list(map(itemgetter('img_path'), val_data)), top_answers)

# # save to disk
##with open('/content/drive/My Drive/Capstone/VQA 2/processed_data/vqa_raw_train2014_top1000.json', 'wb') as f:
##  joblib.dump((questions_train, answer_train, answers_train, images_train), f)
##with open('/content/drive/My Drive/Capstone/VQA 2/processed_data/vqa_raw_val2014_top1000.json', 'wb') as f:
##   joblib.dump((questions_val, answer_val, answers_val, images_val), f)

# loading from disk
with open('/content/drive/My Drive/Capstone/VQA 2/processed_data/vqa_raw_train2014_top1000.json', 'rb') as f:
  questions_train, answer_train, answers_train, images_train = joblib.load(f)

with open('/content/drive/My Drive/Capstone/VQA 2/processed_data/vqa_raw_val2014_top1000.json', 'rb') as f:
  questions_val, answer_val, answers_val, images_val = joblib.load(f)

"""# Creating features

## a) Image features

We will extract image features from the last Conv layer of the VGG19 model. These features are of shape (512, 7, 7).
"""

image_folder = '/train2014/'
if not os.path.exists(os.path.abspath('.') + image_folder):
    image_zip = tf.keras.utils.get_file('train2014.zip',
                                      cache_subdir=os.path.abspath('.'),
                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',
                                      extract = True)
    os.remove(image_zip)

def image_feature_extractor(target_path, image_list, BATCH_SIZE):
	"""
	Extracts (512, 7, 7)-dimensional CNN features and save them locally

	Input:
		target_path: path to save the features
		image_list: image filenames
		BATCH_SIZE: batch size

	Returns:
		None
	"""
	 
	model = VGG19(weights="imagenet", include_top=False, input_tensor=Input(shape=(3, 224, 224)))
  	# add a progress bar
	progbar = utils.Progbar(int(np.ceil(len(image_list) / float(BATCH_SIZE))))

  	# loop over the images in batches
	for (b, i) in enumerate(range(0, len(image_list), BATCH_SIZE)):
		# extract batch of images and prepare them to pass it through the VGG 
		# network for feature extraction

		progbar.update(b+1)
		
		batch_range = range(i, min(i + BATCH_SIZE, len(image_list)))
		batchPaths = image_list[batch_range[0]: batch_range[-1]+1]

		batchImages = []
		batchIds = []
		# loop over the images and labels in the current batch
		for imagePath in batchPaths:

            # load the input image using the Keras helper utility
            # while ensuring the image is resized to 224x224 pixels
			img = image.load_img('train2014/'+imagePath, target_size=(224, 224))
			img = image.img_to_array(img)
    
            # preprocess the image by 
            # (1) expanding the dimensions to include batch dim and
            # (2) subtracting the mean RGB pixel intensity from the ImageNet dataset
			img = np.expand_dims(img, axis=0)
			img = preprocess_input(img)
    
            # add the image to the batch
			batchImages.append(img)
			# image ids of the batch
			batchIds.append(imagePath.split('.')[0][-6:])
	  
		batchImages = np.vstack(batchImages) # (BATCH_SIZE, 3, 224, 224)

		# pass the images through the network and use the outputs as our actual features
		features = model.predict(batchImages) # (BATCH_SIZE, 512, 7, 7)
		features = tf.reshape(features, (features.shape[0], features.shape[1], -1)) # (BATCH_SIZE, 512, 49)
		features = tf.transpose(features, perm =[0,2,1])  # (BATCH_SIZE, 49, 512)

		# loop over the batch to save them locally
		for id, feat in zip(batchIds, features):
			np.save(os.path.join(target_path, id), feat)

image_list = os.listdir('train2014')
BATCH_SIZE = 300
target_path = 'features'

image_feature_extractor(target_path, image_list, BATCH_SIZE)

# size of extracted features
! du -sh features

"""## b) Text features

We will tokenize the question sentences so we can feed them to the Deep Learning Model.
"""

# https://github.com/zcyang/imageqa-san -done
import re

def process_sentence(sentence):
    periodStrip  = re.compile("(?!<=\d)(\.)(?!\d)")
    commaStrip   = re.compile("(\d)(\,)(\d)")
    punct        = [';', r"/", '[', ']', '"', '{', '}',
                    '(', ')', '=', '+', '\\', '_', '-',
                    '>', '<', '@', '`', ',', '?', '!']
    contractions = {"aint": "ain't", "arent": "aren't", "cant": "can't", "couldve": "could've", "couldnt": "couldn't", \
                    "couldn'tve": "couldn't've", "couldnt've": "couldn't've", "didnt": "didn't", "doesnt": "doesn't", "dont": "don't", "hadnt": "hadn't", \
                    "hadnt've": "hadn't've", "hadn'tve": "hadn't've", "hasnt": "hasn't", "havent": "haven't", "hed": "he'd", "hed've": "he'd've", \
                    "he'dve": "he'd've", "hes": "he's", "howd": "how'd", "howll": "how'll", "hows": "how's", "Id've": "I'd've", "I'dve": "I'd've", \
                    "Im": "I'm", "Ive": "I've", "isnt": "isn't", "itd": "it'd", "itd've": "it'd've", "it'dve": "it'd've", "itll": "it'll", "let's": "let's", \
                    "maam": "ma'am", "mightnt": "mightn't", "mightnt've": "mightn't've", "mightn'tve": "mightn't've", "mightve": "might've", \
                    "mustnt": "mustn't", "mustve": "must've", "neednt": "needn't", "notve": "not've", "oclock": "o'clock", "oughtnt": "oughtn't", \
                    "ow's'at": "'ow's'at", "'ows'at": "'ow's'at", "'ow'sat": "'ow's'at", "shant": "shan't", "shed've": "she'd've", "she'dve": "she'd've", \
                    "she's": "she's", "shouldve": "should've", "shouldnt": "shouldn't", "shouldnt've": "shouldn't've", "shouldn'tve": "shouldn't've", \
                    "somebody'd": "somebodyd", "somebodyd've": "somebody'd've", "somebody'dve": "somebody'd've", "somebodyll": "somebody'll", \
                    "somebodys": "somebody's", "someoned": "someone'd", "someoned've": "someone'd've", "someone'dve": "someone'd've", \
                    "someonell": "someone'll", "someones": "someone's", "somethingd": "something'd", "somethingd've": "something'd've", \
                    "something'dve": "something'd've", "somethingll": "something'll", "thats": "that's", "thered": "there'd", "thered've": "there'd've", \
                    "there'dve": "there'd've", "therere": "there're", "theres": "there's", "theyd": "they'd", "theyd've": "they'd've", \
                    "they'dve": "they'd've", "theyll": "they'll", "theyre": "they're", "theyve": "they've", "twas": "'twas", "wasnt": "wasn't", \
                    "wed've": "we'd've", "we'dve": "we'd've", "weve": "we've", "werent": "weren't", "whatll": "what'll", "whatre": "what're", \
                    "whats": "what's", "whatve": "what've", "whens": "when's", "whered": "where'd", "wheres": "where's", "whereve": "where've", \
                    "whod": "who'd", "whod've": "who'd've", "who'dve": "who'd've", "wholl": "who'll", "whos": "who's", "whove": "who've", "whyll": "why'll", \
                    "whyre": "why're", "whys": "why's", "wont": "won't", "wouldve": "would've", "wouldnt": "wouldn't", "wouldnt've": "wouldn't've", \
                    "wouldn'tve": "wouldn't've", "yall": "y'all", "yall'll": "y'all'll", "y'allll": "y'all'll", "yall'd've": "y'all'd've", \
                    "y'alld've": "y'all'd've", "y'all'dve": "y'all'd've", "youd": "you'd", "youd've": "you'd've", "you'dve": "you'd've", \
                    "youll": "you'll", "youre": "you're", "youve": "you've"}

    inText = sentence.replace('\n', ' ')
    inText = inText.replace('\t', ' ')
    inText = inText.strip()
    outText = inText
    for p in punct:
        if (p + ' ' in inText or ' ' + p in inText) or \
           (re.search(commaStrip, inText) != None):
            outText = outText.replace(p, '')
        else:
            outText = outText.replace(p, ' ')
    outText = periodStrip.sub("", outText, re.UNICODE)
    outText = outText.lower().split()
    for wordId, word in enumerate(outText):
        if word in contractions:
            outText[wordId] = contractions[word]
    outText = ' '.join(outText)
    return outText

#done
questions_train_processed = pd.Series(questions_train).apply(process_sentence)
questions_val_processed = pd.Series(questions_val).apply(process_sentence)

"""*NB: More details on text pre-processing can be found in the __VQA 2 EDA__ notebook*"""

# # save to disk -done
with open('/content/drive/My Drive/Capstone/VQA 2/processed_data/processed_questions.pkl', 'wb') as f:
  joblib.dump((questions_train_processed, questions_val_processed), f)

# load from disk
with open('/content/drive/My Drive/Capstone/VQA 2/processed_data/processed_questions.pkl', 'rb') as f:
  questions_train_processed, questions_val_processed = joblib.load(f)

"""__Tokenization:__"""

#done
tok=text.Tokenizer(filters='')
tok.fit_on_texts(questions_train_processed)

"""We pass an empty string to `filters` in __Tokenizer__ because, we do not want to remove any more symbols."""

# # save to disk -done
with open('/content/drive/My Drive/Capstone/VQA 2/text_tokenizer.pkl', 'wb') as f:
  joblib.dump(tok, f)

# load from disk
with open('/content/drive/My Drive/Capstone/VQA 2/text_tokenizer.pkl', 'rb') as f:
  tok = joblib.load(f)

question_data_train = tok.texts_to_sequences(questions_train_processed)
question_data_val = tok.texts_to_sequences(questions_val_processed)

question_len = [len(text) for text in question_data_train]
plt.figure(figsize=(7,5))
sns.distplot(question_len, color='red')
plt.title('Distribution of Question length')
plt.xlabel('Length of Question')
plt.ylabel('Question count')
plt.xlim(0, 30)
plt.show()

for i in range(0,11):
    print(10*i,'percentile value is', np.percentile(question_len,10*i))

for i in range(0,11):
    print(90+i,'percentile value is',np.percentile(question_len,90+i))

"""The max sequence length is 22. So we will pad all the sequences to be of length 22."""

MAX_LEN = 22

question_data_train=sequence.pad_sequences(question_data_train, maxlen=MAX_LEN, padding='post')
question_data_val=sequence.pad_sequences(question_data_val, maxlen=MAX_LEN, padding = 'post')

# # save to disk
with open('/content/drive/My Drive/Capstone/VQA 2/tokenised_data_post.pkl', mode='wb') as f:
  pickle.dump((question_data_train, question_data_val), f)

# load from disk
with open('/content/drive/My Drive/Capstone/VQA 2/tokenised_data_post.pkl', mode='rb') as f:
    question_data_train, question_data_val = pickle.load(f)

"""## c) Answers

Since, we have posed the problem statement at hand as a 1000 class classification problem, we one-hot encode the labels (ie. answers).
"""

labelencoder = preprocessing.LabelEncoder()
labelencoder.fit(answer_train)

len(labelencoder.classes_)

# # save to disk
with open('/content/drive/My Drive/Capstone/VQA 2/labelencoder.pkl', 'wb') as f:
  joblib.dump(labelencoder, f)

# load from disk
with open('/content/drive/My Drive/Capstone/VQA 2/labelencoder.pkl', 'rb') as f:
  labelencoder = joblib.load(f)

def get_answers_matrix(answers, encoder):
	'''
	One-hot-encodes the answers

	Input:
		answers:	list of answer
		encoder:	a scikit-learn LabelEncoder object
  
	Output:
		A numpy array of shape (# of answers, # of class)
	'''
	y = encoder.transform(answers) #string to numerical class
	nb_classes = encoder.classes_.shape[0]
	Y = utils.to_categorical(y, nb_classes)
	return Y

"""# Preparing the data matrices:

Splitting the data to train and validation:
"""

sss = StratifiedShuffleSplit(n_splits=1, test_size= 0.25,random_state=42)

for train_index, val_index in sss.split(images_train, answer_train):
  TRAIN_INDEX = train_index
  VAL_INDEX = val_index

# image data
image_list_tr, image_list_vl = np.array(images_train)[TRAIN_INDEX.astype(int)], np.array(images_train)[VAL_INDEX.astype(int)]

# question data
question_tr, question_vl = question_data_train[TRAIN_INDEX], question_data_train[VAL_INDEX]

# answer data
answer_matrix = get_answers_matrix(answer_train, labelencoder)
answer_tr, answer_vl = answer_matrix[TRAIN_INDEX], answer_matrix[VAL_INDEX]

"""# Model

The model is an implemenatation of the paper [Hierarchical Question-Image Co-Attention for Visual Question Answering](https://arxiv.org/pdf/1606.00061).

<h2>Paper Overview:</h2>

![Hie Co Att Architecture Image](https://i.imgur.com/P8iaYh9.jpg)


All the papers on VQA before this, focused mainly on visual attention. This paper proposes to focus on question attention too. Specifically, this paper presents a novel multi-modal attention model for VQA
with the following two unique features:



1.   **Co-Attention:** : This paper proposes a novel mechanism to jointly reasons about visual attention and question attention, which is referred to as co-attention. More specifically, the image representation is used to guide the question attention and the question representation(s) are
used to guide image attention.

2.   **Question Hierarchy:** Builds a hierarchical architecture that co-attends to the image and question
at three levels: (a) word level, (b) phrase level and (c) question level. 

a) **At the word level**, the words are embedded to a vector space through an embedding matrix. 

b) **At the phrase level**, 1-dimensional convolution
neural networks are used on the word representations with temporal filters of varying support, to capture the information contained in unigrams, bigrams and trigrams, and then combine the various n-gram responses by pooling them into a single phrase level representation.

c) **At the question level**, a recurrent neural networks is used to encode the entire question.

For each level of the question representation in this hierarchy, joint question and image co-attention maps are constructed, which are then combined recursively to ultimately predict a distribution over the answers.

<h2>Method:</h2>

The paper proposes two co-attention mechanisms that differ in the order in which image and question
attention maps are generated. The first mechanism, which is called **parallel co-attention**, it generates image and question attention simultaneously. The second mechanism, is called **alternating co-attention**, it sequentially alternates between generating image and question attentions.
These co-attention mechanisms are executed at all three levels of the question hierarchy.

For our work, we will implement the Parallel Co-Attention model.

<h2>Parallel Co-Attention</h2>

![Parallel Co-Attention Arch Image](https://i.imgur.com/7EVJNI9.jpg)


Parallel co-attention attends to the image and question simultaneously. The image and question are connected by calculating the similarity between image and question features at all pairs of image-locations and question-locations. Specifically, given an image
feature map $V ∈ R^{d×N}$, and the question representation $Q ∈ R^{d×T}$, we calculate something called affinity matrix $C ∈ R^{T ×N}$ as follows:

$~~~~~~~~~~~~~~$ $C = tanh(Q^{T}W_bV)$


Considering this affinity matrix $C$ as a feature, image and question attention maps are predicted in the following way:

- $Hv = tanh(W_vV + (W_qQ)C)$ $~~$,$~~~~~~$ $Hq = tanh(W_qQ + (W_vV )C^T)$

- $a^v = softmax(w^{T}_{hv}H^v)$ $~~~~~~~~~~~~~$, $~~~~~$ $a^q = softmax(w^{T}_{hq}H^q)$


where, $W_v, W_q ∈ R^{k×d}, w_{hv}, w_{hq} ∈ R^k$ are the weight parameters, <br>
$~~~~~~~~~~ a^v ∈ R^N$ and $a^q ∈ R^T$ are the attention probabilities of each image regions and the words respectively. <br> 


Based on the above attention weights, the image and question attention vectors are calculated as the weighted sum of the image features and question features, i.e., <br>

- $ v = \sum_{n=1}^{N} a_{n}^{v}v_n$ $~~~~~~$,$~~~~~~$ $ q = \sum_{t=1}^{T} a_{t}^{q}q_t$


The parallel co-attention is done at each level in the hierarchy, leading to $v^{r}$ and $q^r$ where $r ∈$
{$w, p, s$}.

The co-attended image and question features from all three levels are then combined recursively to ultimately predict the answer.

![Answer Prediction](https://i.imgur.com/MlOKCjV.jpg)


$h^w = tanh(W_w(q^w + v^w))$ <br>
$h^p = tanh(W_p[(q^p + v^p), ~ h^w])$ <br>
$h^s = tanh(W_s[(q^s + v^s), ~ h^p])$ <br>
$p = softmax(W_{h}h^s)$ <br>
where $W_w, W_p, W_s$ and $W_h$ are the weight parameters. <br>
$ ~~~~~~~~~~ [·]$ is the concatenation operation on two
vectors. <br>
$~~~~~~~~~~ p$ is the probability of the final answer.

## Custom Layers:

*NB: The variable have been named as per the paper (wherever possible).*

*Code Ref: https://github.com/ritvikshrivastava/ADL_VQA_Tensorflow2*

*Weights Initialization Ref: https://stats.stackexchange.com/a/393012*
"""

class AttentionMaps(tf.keras.layers.Layer):
  """
  Given an image feature map V ∈ R(d×N), and the question representation Q ∈ R(d×T), 
  calculates the affinity matrix C ∈ R(T×N): C = tanh((QT)(Wb)V) ; 
  where Wb ∈ R(d×d) contains the weights. (Refer eqt (3) section 3.3).

  Given this affinity matrix C ∈ R(T×N), predicts image and question attention maps 
  (Refer eqt (4) section 3.3).

  Arguments:
    dim_k     : hidden attention dimention
    reg_value : Regularization value


  Inputs:
    image_feat,    V : shape (N,  d) or (49, dim_d)
    ques_feat,     Q : shape (T,  d) or (23, dim_d)

  Outputs:
    Image and Question attention maps viz:
    a) Hv = tanh(WvV + (WqQ)C) and
    b) Hq = tanh(WqQ + (WvV )CT)
  """
  def __init__(self, dim_k, reg_value, **kwargs):
    super(AttentionMaps, self).__init__(**kwargs)

    self.dim_k = dim_k
    self.reg_value = reg_value

    self.Wv = Dense(self.dim_k, activation=None,\
                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\
                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=2))
    self.Wq = Dense(self.dim_k, activation=None,\
                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\
                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=3))

  def call(self, image_feat, ques_feat):
    """
    The main logic of this layer.
    """  

    # Affinity Matrix C
    # (QT)(Wb)V 
    C = tf.matmul(ques_feat, tf.transpose(image_feat, perm=[0,2,1])) # [b, 23, 49]
    # tanh((QT)(Wb)V)
    C = tf.keras.activations.tanh(C) 

    # (Wv)V
    WvV = self.Wv(image_feat)                             # [b, 49, dim_k]
    # (Wq)Q
    WqQ = self.Wq(ques_feat)                              # [b, 23, dim_k]

    # ((Wq)Q)C
    WqQ_C = tf.matmul(tf.transpose(WqQ, perm=[0,2,1]), C) # [b, k, 49]
    WqQ_C = tf.transpose(WqQ_C, perm =[0,2,1])            # [b, 49, k]

    # ((Wv)V)CT                                           # [b, k, 23]
    WvV_C = tf.matmul(tf.transpose(WvV, perm=[0,2,1]), tf.transpose(C, perm=[0,2,1]))  
                        
    WvV_C = tf.transpose(WvV_C, perm =[0,2,1])            # [b, 23, k]

    #---------------image attention map------------------
    # We find "Hv = tanh((Wv)V + ((Wq)Q)C)" ; H_v shape [49, k]

    H_v = WvV + WqQ_C                                     # (Wv)V + ((Wq)Q)C
    H_v = tf.keras.activations.tanh(H_v)                  # tanh((Wv)V + ((Wq)Q)C) 

    #---------------question attention map---------------
    # We find "Hq = tanh((Wq)Q + ((Wv)V)CT)" ; H_q shape [23, k]

    H_q = WqQ + WvV_C                                     # (Wq)Q + ((Wv)V)CT
    H_q = tf.keras.activations.tanh(H_q)                  # tanh((Wq)Q + ((Wv)V)CT) 
        
    return [H_v, H_q]                                     # [b, 49, k], [b, 23, k]
  
  def get_config(self):
    """
    This method collects the input shape and other information about the layer.
    """
    config = {
        'dim_k': self.dim_k,
        'reg_value': self.reg_value
    }
    base_config = super(AttentionMaps, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

layer = AttentionMaps(64, 0.001)
config = layer.get_config()
print(config)
new_layer = AttentionMaps.from_config(config)

class ContextVector(tf.keras.layers.Layer):
  """
  Method to find context vector of the image and text features
  (Refer eqt (4) and (5) section 3.3).
  
  Arguments:
    reg_value : Regularization value
    
  Inputs:
    image_feat V: image features, (49, d)
    ques_feat  Q: question features, (23, d)
    H_v: image attention map, (49, k)
    H_q: question attention map, (23, k)

  Outputs:
    Returns d-dimenstional context vector for image and question features
  """
  def __init__(self, reg_value, **kwargs):
    super(ContextVector, self).__init__(**kwargs)

    self.reg_value = reg_value

    self.w_hv = Dense(1, activation='softmax',\
                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\
                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=4))
    self.w_hq = Dense(1, activation='softmax',\
                        kernel_regularizer=tf.keras.regularizers.l2(self.reg_value),\
                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)) 
    

  def call(self, image_feat, ques_feat, H_v, H_q):
    """
    The main logic of this layer.
    """  
    # attention probabilities of each image region vn; a_v = softmax(wT_hv * H_v)
    a_v = self.w_hv(H_v)                               # [b, 49, 1]

    # attention probabilities of each word qt ;        a_q = softmax(wT_hq * H_q)
    a_q = self.w_hq(H_q)                               # [b, 23, 1]

    # context vector for image
    v = a_v * image_feat                               # [b, 49, dim_d]
    v = tf.reduce_sum(v, 1)                            # [b, dim_d]

    # context vector for question
    q = a_q * ques_feat                                # [b, 23, dim_d]
    q = tf.reduce_sum(q, 1)                            # [b, dim_d]


    return [v, q]

  def get_config(self):
    """
    This method collects the input shape and other information about the layer.
    """
    config = {
        'reg_value': self.reg_value
    }
    base_config = super(ContextVector, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

layer = ContextVector(0.001)
config = layer.get_config()
print(config)
new_layer = ContextVector.from_config(config)

class PhraseLevelFeatures(tf.keras.layers.Layer):
  """
  We compute the phrase features by applying 1-D convolution on the word embedding 
  vectors with filters of three window sizes: unigram, bigram and trigram.
  The word-level features Qw are appropriately 0-padded before feeding into bigram and 
  trigram convolutions to maintain the length of the sequence after convolution.
  Given the convolution result, we then apply max-pooling across different n-grams at each word
  location to obtain phrase-level features
  (Refer eqt (1) and (2) section 3.2).

  Arguments:
    dim_d: hidden dimension

  Inputs:
    word_feat Q : word level features of shape (23, dim_d)

  Outputs:
    Phrase level features of the question of shape (23, dim_d)
  """
  def __init__(self, dim_d, **kwargs):
    super(PhraseLevelFeatures, self).__init__(**kwargs)
    
    self.dim_d = dim_d
    
    self.conv_unigram = Conv1D(self.dim_d, kernel_size=1, strides=1,\
                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=6)) 
    self.conv_bigram =  Conv1D(self.dim_d, kernel_size=2, strides=1, padding='same',\
                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=7)) 
    self.conv_trigram = Conv1D(self.dim_d, kernel_size=3, strides=1, padding='same',\
                            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=8)) 


  def call(self, word_feat):
    """
    The main logic of this layer.

    Compute the n-gram phrase embeddings (n=1,2,3)
    """
    # phrase level unigram features
    x_uni = self.conv_unigram(word_feat)                    # [b, 23, dim_d]

    # phrase level bigram features
    x_bi  = self.conv_bigram(word_feat)                     # [b, 23, dim_d]

    # phrase level trigram features
    x_tri = self.conv_trigram(word_feat)                    # [b, 23, dim_d]

    # Concat
    x = tf.concat([tf.expand_dims(x_uni, -1),\
                    tf.expand_dims(x_bi, -1),\
                    tf.expand_dims(x_tri, -1)], -1)         # [b, 23, dim_d, 3]

    # https://stackoverflow.com/a/36853403
    # Max-pool across n-gram features; over-all phrase level feature
    x = tf.reduce_max(x, -1)                                # [b, 23, dim_d]

    return x

  def get_config(self):
    """
    This method collects the input shape and other information about the layer.
    """
    config = {
        'dim_d': self.dim_d
    }
    base_config = super(PhraseLevelFeatures, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

layer = PhraseLevelFeatures(32)
config = layer.get_config()
print(config)
new_layer = PhraseLevelFeatures.from_config(config)

"""## Architecture:

The model architecture is an implemenatation of the paper [Hierarchical Question-Image Co-Attention for Visual Question Answering](https://arxiv.org/pdf/1606.00061).
"""

def build_model(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value):
    """
    Defines the Keras model.

    Arguments
    ----------
    max_answers : Number of output targets of the model.
    max_seq_len : Length of input sequences
    vocab_size  : Size of the vocabulary, i.e. maximum integer index + 1.
    dim_d       : Hidden dimension
    dim_k       : Hidden attention dimension
    l_rate      : Learning rate for the model
    d_rate      : Dropout rate
    reg_value   : Regularization value

    Returns
    ----------
    Returns the Keras model.
    """
    # inputs 
    image_input = Input(shape=(49, 512, ), name='Image_Input')
    ques_input = Input(shape=(22, ), name='Question_Input')

    # image feature; (Wb)V                                          # [b, 49, dim_d]
    image_feat = Dense(dim_d, activation=None, name='Image_Feat_Dense',\
                            kernel_regularizer=tf.keras.regularizers.l2(reg_value),\
                                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=1))(image_input)
    image_feat = Dropout(d_rate, seed=1)(image_feat)

    # word level
    ques_feat_w = Embedding(input_dim=vocab_size, output_dim=dim_d, input_length=max_seq_len,\
                            mask_zero=True)(ques_input)
    
    Hv_w, Hq_w = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Word')(image_feat, ques_feat_w)
    v_w, q_w = ContextVector(reg_value, name='ContextVector_Word')(image_feat, ques_feat_w, Hv_w, Hq_w)
    feat_w = tf.add(v_w,q_w)
    h_w = Dense(dim_d, activation='tanh', name='h_w_Dense',\
                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\
                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=13))(feat_w)

    # phrase level
    ques_feat_p = PhraseLevelFeatures(dim_d, name='PhraseLevelFeatures')(ques_feat_w)

    Hv_p, Hq_p = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Phrase')(image_feat, ques_feat_p)
    v_p, q_p = ContextVector(reg_value, name='ContextVector_Phrase')(image_feat, ques_feat_p, Hv_p, Hq_p)
    feat_p = concatenate([tf.add(v_p,q_p), h_w], -1)
    h_p = Dense(dim_d, activation='tanh', name='h_p_Dense',\
                kernel_regularizer=tf.keras.regularizers.l2(reg_value),\
                kernel_initializer=tf.keras.initializers.glorot_uniform(seed=14))(feat_p)

    # sentence level
    #tf.expand_dims(ques_feat_p,1,name=None)
    #ques_feat_p.set_shape((1,22,512))
    print(ques_feat_p)
    ques_feat_s = LSTM(dim_d, return_sequences=True, input_shape=(None, max_seq_len, dim_d),\
                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(ques_feat_p)

    Hv_s, Hq_s = AttentionMaps(dim_k, reg_value, name='AttentionMaps_Sent')(image_feat, ques_feat_s)
    v_s, q_s = ContextVector(reg_value, name='ContextVector_Sent')(image_feat, ques_feat_p, Hv_s, Hq_s)
    feat_s = concatenate([tf.add(v_s,q_s), h_p], -1) 
    h_s = Dense(2*dim_d, activation='tanh', name='h_s_Dense',\
                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\
                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=15))(feat_s)

    z   = Dense(2*dim_d, activation='tanh', name='z_Dense',\
                    kernel_regularizer=tf.keras.regularizers.l2(reg_value),\
                        kernel_initializer=tf.keras.initializers.glorot_uniform(seed=16))(h_s)
    z   = Dropout(d_rate, seed=16)(z)

    # result
    result = Dense(max_answers, activation='softmax')(z)

    model = Model(inputs=[image_input, ques_input], outputs=result)

    return model

"""# Create tf.Dataset:"""

BATCH_SIZE = 300
BUFFER_SIZE = 5000

# loading the numpy files
def map_func(img_name, ques, ans):
    img_tensor = np.load('features/' + img_name.decode('utf-8').split('.')[0][-6:] + '.npy')
    return img_tensor, ques, ans

dataset_tr = tf.data.Dataset.from_tensor_slices((image_list_tr, question_tr, answer_tr))

# Use map to load the numpy files in parallel
dataset_tr = dataset_tr.map(lambda item1, item2, item3: tf.numpy_function(
    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),
    num_parallel_calls=tf.data.experimental.AUTOTUNE)

# Shuffle and batch
dataset_tr = dataset_tr.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
dataset_tr = dataset_tr.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

print(dataset_tr)

dataset_vl = tf.data.Dataset.from_tensor_slices((image_list_vl, question_vl, answer_vl))

# Use map to load the numpy files in parallel
dataset_vl = dataset_vl.map(lambda item1, item2, item3: tf.numpy_function(
    map_func, [item1, item2, item3], [tf.float32, tf.int32, tf.float32]),
    num_parallel_calls=tf.data.experimental.AUTOTUNE)

# Shuffle and batch
dataset_vl = dataset_vl.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
dataset_vl = dataset_vl.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

"""# Training:

Create the model:
"""

# params 1
max_answers = 1000
max_seq_len = 22
vocab_size  = len(tok.word_index) + 1
EPOCHS      = 90

dim_d       = 512
dim_k       = 256
l_rate      = 1e-4
d_rate      = 0.5
reg_value   = 0.01

base_path = '/content/drive/My Drive/Capstone/VQA 2/temps'

# create model
model = build_model(max_answers, max_seq_len, vocab_size, dim_d, dim_k, l_rate, d_rate, reg_value)

"""**TRIAL**"""

model.summary()

tf.keras.utils.plot_model(model,to_file="model.png", show_shapes=True, show_layer_names=True,rankdir='TB',
expand_nested=False,dpi=96)

"""Choose loss and optimizer:"""

steps_per_epoch = int(np.ceil(len(image_list_tr)/BATCH_SIZE))
boundaries      = [50*steps_per_epoch]
values          = [l_rate, l_rate/10]

# we reduce the l_rate after 50th epoch (from 1e-4 to 1e-5)
learning_rate_fn = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)
optimizer        = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn)

loss_object      = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction='auto')

"""Create the checkpoint objects:"""

checkpoint_directory = base_path+"/training_checkpoints/"+str(l_rate)+"_"+str(dim_k)
SAVE_CKPT_FREQ = 5

print(checkpoint_directory)

ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(ckpt, checkpoint_directory, max_to_keep=3)

"""Create stateful metrics that can be used to accumulate values during training and logged at any point:"""

train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)

train_score = F1Score(num_classes=max_answers, average='micro', name='train_score')
val_score = F1Score(num_classes=max_answers, average='micro', name='val_score')

"""Configure tensorboard:"""

train_log_dir = base_path+'/logs/'+str(l_rate)+"_"+str(dim_k)+'/train'
val_log_dir   = base_path+'/logs/'+str(l_rate)+"_"+str(dim_k)+'/validation'

train_summary_writer = tf.summary.create_file_writer(train_log_dir)
val_summary_writer = tf.summary.create_file_writer(val_log_dir)

"""Define the training and test functions:"""

# @tf.function
def train_step(model, img, ques, ans, optimizer):
  with tf.GradientTape() as tape:
    # forward pass
    predictions = model([img, ques], training=True)
    loss = loss_object(ans, predictions)

  # backward pass
  grads = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(grads, model.trainable_variables))

  # record results
  train_loss(loss)
  train_score(ans, predictions)

  # all gradients
  grads_ = list(zip(grads, model.trainable_variables))
  return grads_

def test_step(model, img, ques, ans):
  predictions = model([img, ques])
  loss = loss_object(ans, predictions)

  # record results
  val_loss(loss)
  val_score(ans, predictions)

"""Start Training:"""

if manager.latest_checkpoint:
    ckpt.restore(manager.latest_checkpoint)
    print("Restored from {}".format(manager.latest_checkpoint))
    START_EPOCH = int(manager.latest_checkpoint.split('-')[-1]) * SAVE_CKPT_FREQ
    print("Resume training from epoch: {}".format(START_EPOCH))
else:
    print("Initializing from scratch")
    START_EPOCH = 0
EPOCHS = 90

for epoch in range(START_EPOCH, EPOCHS):

  start = time.time()

  for img, ques, ans in (dataset_tr):
    grads = train_step(model, img, ques, ans, optimizer)

  # tensorboard  
  with train_summary_writer.as_default():
    # Create a summary to monitor cost tensor
    tf.summary.scalar('loss', train_loss.result(), step=epoch)
    # Create a summary to monitor accuracy tensor
    tf.summary.scalar('f1_score', train_score.result(), step=epoch)
    # Create summaries to visualize weights
    for var in model.trainable_variables:
        tf.summary.histogram(var.name, var, step=epoch)
    # Summarize all gradients
    for grad, var in grads:
        tf.summary.histogram(var.name + '/gradient', grad, step=epoch)

  for img, ques, ans in (dataset_vl):
    test_step(model, img, ques, ans)
  
  # tensorboard
  with val_summary_writer.as_default():
    # Create a summary to monitor cost tensor
    tf.summary.scalar('loss', val_loss.result(), step=epoch)
    # Create a summary to monitor accuracy tensor
    tf.summary.scalar('f1_score', val_score.result(), step=epoch)
  
  template = 'Epoch {}, loss: {:.4f}, f1_score: {:.4f}, val loss: {:.4f}, val f1_score: {:.4f}, time: {:.0f} sec'
  print (template.format(epoch + 1,
                         train_loss.result(), 
                         train_score.result(),
                         val_loss.result(), 
                         val_score.result(),
                         (time.time() - start)))

  # Reset metrics every epoch
  train_loss.reset_states()
  train_score.reset_states()
  val_loss.reset_states()
  val_score.reset_states()

  # save checkpoint every SAVE_CKPT_FREQ step
  ckpt.step.assign_add(1)
  if int(ckpt.step) % SAVE_CKPT_FREQ == 0:
      manager.save()
      print('Saved checkpoint.')

"""Start TensorBoard:"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

"""Download files: [Here](https://drive.google.com/drive/folders/1-KJrmaCNMEvjevlVTvvkWdLGeQz_MmAX?usp=sharing)"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir '/content/drive/My Drive/Capstone/VQA 2/temps/logs/0.0001_256'

"""Visualize plots:

![Image for LOSS](https://i.imgur.com/ND1nxe3.jpg)

![Image for F1-SCORE](https://i.imgur.com/qPYWw7H.jpg)

# Evaluation:

Prepare data:
"""

tr_questions = np.array(questions_train_processed)[TRAIN_INDEX.astype(int)]
val_questions = np.array(questions_train_processed)[VAL_INDEX.astype(int)]

tr_answers = np.array(answers_train)[TRAIN_INDEX.astype(int)]
val_answers = np.array(answers_train)[VAL_INDEX.astype(int)]

tr_answer = np.array(answer_train)[TRAIN_INDEX.astype(int)]
val_answer = np.array(answer_train)[VAL_INDEX.astype(int)]

tr_images = np.array(images_train)[TRAIN_INDEX.astype(int)]
val_images = np.array(images_train)[VAL_INDEX.astype(int)]

# loading the numpy files
def map_func_eval(img_name, ques):
    img_tensor = np.load('features/' + img_name.decode('utf-8').split('.')[0][-6:] + '.npy')
    return img_tensor, ques

dataset_tr_eval = tf.data.Dataset.from_tensor_slices((image_list_tr, question_tr))

# Use map to load the numpy files in parallel
dataset_tr_eval = dataset_tr_eval.map(lambda item1, item2: tf.numpy_function(
    map_func_eval, [item1, item2], [tf.float32, tf.int32]),
    num_parallel_calls=tf.data.experimental.AUTOTUNE)

# batch
dataset_tr_eval = dataset_tr_eval.batch(BATCH_SIZE)

dataset_vl_eval = tf.data.Dataset.from_tensor_slices((image_list_vl, question_vl))

# Use map to load the numpy files in parallel
dataset_vl_eval = dataset_vl_eval.map(lambda item1, item2: tf.numpy_function(
    map_func_eval, [item1, item2], [tf.float32, tf.int32]),
    num_parallel_calls=tf.data.experimental.AUTOTUNE)

# batch
dataset_vl_eval = dataset_vl_eval.batch(BATCH_SIZE)

"""Prediction:"""

model1 = load_model('/content/drive/My Drive/Capstone/VQA 2/model.h5', custom_objects={'AttentionMaps': AttentionMaps, 'ContextVector':ContextVector, 'PhraseLevelFeatures':PhraseLevelFeatures})

def predict_answers(dataset, model, labelencoder):
    """
    Prediction function
    """
    predictions = []
    for img, ques in (dataset):
        preds = model([img, ques])
        predictions.extend(preds)

    y_classes = tf.argmax(predictions, axis=1, output_type=tf.int32)
    y_predict = (labelencoder.inverse_transform(y_classes))
    return y_predict

# predict answers for the train set
y_predict_text_tr = predict_answers(dataset_tr_eval, model, labelencoder)
# predict answers for the validation set
y_predict_text_vl = predict_answers(dataset_vl_eval, model, labelencoder)

"""Score:"""

model.save("/content/drive/My Drive/Capstone/VQA 2/model90.h5")

def model_metric(predictions, truths):
    """
    Measures the accuracy of the predictions

    Input:
        predictions : predictions
        truths      : ground truth answers

    Returns:
        Accuracy measure of the model
    """

    total = 0
    correct_val=0.0

    for prediction, truth in zip(predictions, truths):
        
        temp_count=0
        total +=1

        for _truth in truth.split(';'):
            if prediction == _truth:
                temp_count+=1
        
        # accuracy = min((# humans that provided that answer/3) , 1)
        if temp_count>2:
            correct_val+=1
        else:
            correct_val+=float(temp_count)/3

    return (correct_val/total)*100

tr_score = model_metric(y_predict_text_tr, tr_answers)

print('Final Accuracy on the train set is', tr_score)

val_score = model_metric(y_predict_text_vl, val_answers)

print('Final Accuracy on the validation set is', val_score)





"""# Inference:

Let's look at for which type of outputs the model is working and for which type it is not:
"""

from collections import Counter

"""## Error Analysis on Train Data:"""

data_true_q = []
data_true_a = []
data_false_q = []
data_false_a = []

for (prediction, truth, question) in (zip(y_predict_text_tr, tr_answer, tr_questions)):
  # correct prediction
  if (prediction == truth):
    data_true_q.append(question)
    data_true_a.append(truth)
  else: # incorrect prediction
    data_false_q.append(question)
    data_false_a.append(truth)

# question type counts
question_type_true = pd.Series(data_true_q).apply(lambda x: x.split()[0])
question_type_false = pd.Series(data_false_q).apply(lambda x: x.split()[0])

question_type_true_c = Counter(question_type_true)
question_type_false_c = Counter(question_type_false)

sorted_freq_true_q = sorted(question_type_true_c.items(), key=operator.itemgetter(1), reverse=True)
sorted_freq_false_q = sorted(question_type_false_c.items(), key=operator.itemgetter(1), reverse=True)

# answer type counts
answer_type_true_c = Counter(data_true_a)
answer_type_false_c = Counter(data_false_a)

sorted_freq_true_a = sorted(answer_type_true_c.items(), key=operator.itemgetter(1), reverse=True)
sorted_freq_false_a = sorted(answer_type_false_c.items(), key=operator.itemgetter(1), reverse=True)

"""Question type of the top 20 correctly answered questions:"""

sorted_freq_true_q[:20]

"""Question type of the top 20 in-correctly answered questions:"""

sorted_freq_false_q[:20]

"""Answer type of the top 20 correctly answered questions:"""

sorted_freq_true_a[:20]

"""Answer type of the top 20 in-correctly answered questions:"""

sorted_freq_false_a[:20]

"""## Error Analysis on Validation data:"""

data_true_q = []
data_true_a = []
data_false_q = []
data_false_a = []

for (prediction, truth, question) in (zip(y_predict_text_vl, val_answer, val_questions)):
  # correct prediction
  if (prediction == truth):
    data_true_q.append(question)
    data_true_a.append(truth)
  else: # incorrect prediction
    data_false_q.append(question)
    data_false_a.append(truth)

# question type counts
question_type_true = pd.Series(data_true_q).apply(lambda x: x.split()[0])
question_type_false = pd.Series(data_false_q).apply(lambda x: x.split()[0])

question_type_true_c = Counter(question_type_true)
question_type_false_c = Counter(question_type_false)

sorted_freq_true_q = sorted(question_type_true_c.items(), key=operator.itemgetter(1), reverse=True)
sorted_freq_false_q = sorted(question_type_false_c.items(), key=operator.itemgetter(1), reverse=True)

# answer type counts
answer_type_true_c = Counter(data_true_a)
answer_type_false_c = Counter(data_false_a)

sorted_freq_true_a = sorted(answer_type_true_c.items(), key=operator.itemgetter(1), reverse=True)
sorted_freq_false_a = sorted(answer_type_false_c.items(), key=operator.itemgetter(1), reverse=True)

"""Question type of the top 20 correctly answered questions:"""

sorted_freq_true_q[:20]

"""Question type of the top 20 in-correctly answered questions:"""

sorted_freq_false_q[:20]

"""Answer type of the top 20 correctly answered questions:"""

sorted_freq_true_a[:20]

"""Answer type of the top 20 in-correctly answered questions:"""

sorted_freq_false_a[:20]

"""# Conclusion:

The performance of the model is:

<table>
  <tr>
    <th>Base LR</th>
    <th>dim_d</th>
    <th>dim_k</th>
    <th>Tr Accuaray</th>
    <th>Val Accuracy</th>
  </tr>
  <tr>
    <td>1e-4</td>
    <td>512</td>
    <td>256</td>
    <td>54.7897</td>
    <td>49.2800</td>
  </tr>
</table>
"""